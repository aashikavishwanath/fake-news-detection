{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRZe6dBFbKhQabnzj/FG2O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_1GS8HLEQxY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "import pickle\n",
        "\n",
        "basepath = '.'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "with open(os.path.join(basepath, 'train_val_data.pkl'), 'rb') as f:\n",
        "  train_data, val_data = pickle.load(f)\n",
        "\n",
        "print('Number of train examples:', len(train_data))\n",
        "print('Number of val examples:', len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_description_from_html(html):\n",
        "  soup = bs(html)\n",
        "  description_tag = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})\n",
        "  if description_tag:\n",
        "    description = description_tag.get('content') or ''\n",
        "  else: # If there is no description, return empty string.\n",
        "    description = ''\n",
        "  return description\n",
        "\n",
        "def scrape_description(url):\n",
        "  if not url.startswith('http'):\n",
        "    url = 'http://' + url\n",
        "  response = requests.get(url, timeout=10)\n",
        "  html = response.text\n",
        "  description = get_description_from_html(html)\n",
        "  return description\n",
        "\n",
        "print('Description of Google.com:')\n",
        "print(scrape_description('google.com'))"
      ],
      "metadata": {
        "id": "HI6JwXd_Epal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"youtube.com\"\n",
        "print('Description of %s:' % url)\n",
        "print(scrape_description(url))"
      ],
      "metadata": {
        "id": "F6QqaxTSErWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_descriptions_from_data(data):\n",
        "  # A dictionary mapping from url to description for the websites in\n",
        "  # train_data.\n",
        "  descriptions = []\n",
        "  for site in tqdm(data):\n",
        "    url, html, label = site\n",
        "    descriptions.append(get_description_from_html(html))\n",
        "  return descriptions\n",
        "\n",
        "\n",
        "train_descriptions = get_descriptions_from_data(train_data)\n",
        "train_urls = [url for (url, html, label) in train_data]\n",
        "\n",
        "print('\\nNYTimes Description:')\n",
        "print(train_descriptions[train_urls.index('nytimes.com')])"
      ],
      "metadata": {
        "id": "UCGmp7agEuZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_descriptions = get_descriptions_from_data(val_data)"
      ],
      "metadata": {
        "id": "vN_U9SwZE0KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=300)\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(descriptions, vectorizer):\n",
        "  X = vectorizer.transform(descriptions).todense()\n",
        "  return X\n",
        "\n",
        "print('\\nPreparing train data...')\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_train_y = [label for url, html, label in train_data]\n",
        "\n",
        "print('\\nPreparing val data...')\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "bow_val_y = [label for url, html, label in val_data]\n"
      ],
      "metadata": {
        "id": "IfV-FBJvE1i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(bow_train_X, bow_train_y)\n",
        "\n",
        "bow_train_y_pred = model.predict(bow_train_X)\n",
        "print('Train accuracy', accuracy_score(bow_train_y, bow_train_y_pred))\n",
        "\n",
        "bow_val_y_pred = model.predict(bow_val_X)\n",
        "print('Val accuracy', accuracy_score(bow_val_y, bow_val_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(bow_val_y, bow_val_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(bow_val_y, bow_val_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])"
      ],
      "metadata": {
        "id": "a2BJ04dCE4ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "# Returns word vector for word if it exists, else return None.\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None"
      ],
      "metadata": {
        "id": "WIPqFVsaE8A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_vector = get_word_vector('good')\n",
        "print('Shape of good vector:', good_vector.shape)\n",
        "print(good_vector)"
      ],
      "metadata": {
        "id": "NfRE7LzXFATJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "word1 = \"good\"\n",
        "word2 = \"great\"\n",
        "\n",
        "print('Word 1:', word1)\n",
        "print('Word 2:', word2)\n",
        "\n",
        "def cosine_similarity_of_words(word1, word2):\n",
        "  vec1 = get_word_vector(word1)\n",
        "  vec2 = get_word_vector(word2)\n",
        "\n",
        "  if vec1 is None:\n",
        "    print(word1, 'is not a valid word. Try another.')\n",
        "  if vec2 is None:\n",
        "    print(word2, 'is not a valid word. Try another.')\n",
        "  if vec1 is None or vec2 is None:\n",
        "    return None\n",
        "\n",
        "  return cosine_similarity(vec1, vec2)\n",
        "\n",
        "\n",
        "print('\\nCosine similarity:', cosine_similarity_of_words(word1, word2))\n"
      ],
      "metadata": {
        "id": "8qa7_8YkFEEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split():\n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "\n",
        "    return X\n",
        "\n",
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_train_y = [label for (url, html, label) in train_data]\n",
        "\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "glove_val_y = [label for (url, html, label) in val_data]"
      ],
      "metadata": {
        "id": "Xv6FIFnWFIiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(glove_train_X, glove_train_y)\n",
        "\n",
        "glove_train_y_pred = model.predict(glove_train_X)\n",
        "print('Train accuracy', accuracy_score(glove_train_y, glove_train_y_pred))\n",
        "\n",
        "glove_val_y_pred = model.predict(glove_val_X)\n",
        "print('Val accuracy', accuracy_score(glove_val_y, glove_val_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(glove_val_y, glove_val_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(glove_val_y, glove_val_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])"
      ],
      "metadata": {
        "id": "5JKJm3F8FQb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_X, train_y, val_X, val_y):\n",
        "  model = LogisticRegression(solver='liblinear')\n",
        "  model.fit(train_X, train_y)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(train_X, train_y, val_X, val_y):\n",
        "  model = train_model(train_X, train_y, val_X, val_y)\n",
        "\n",
        "  train_y_pred = model.predict(train_X)\n",
        "  print('Train accuracy', accuracy_score(train_y, train_y_pred))\n",
        "\n",
        "  val_y_pred = model.predict(val_X)\n",
        "  print('Val accuracy', accuracy_score(val_y, val_y_pred))\n",
        "\n",
        "  print('Confusion matrix:')\n",
        "  print(confusion_matrix(val_y, val_y_pred))\n",
        "\n",
        "  prf = precision_recall_fscore_support(val_y, val_y_pred)\n",
        "\n",
        "  print('Precision:', prf[0][1])\n",
        "  print('Recall:', prf[1][1])\n",
        "  print('F-Score:', prf[2][1])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "iax_JHEwFRkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data, featurizer):\n",
        "    X = []\n",
        "    y = []\n",
        "    for datapoint in data:\n",
        "        url, html, label = datapoint\n",
        "\n",
        "        html = html.lower()\n",
        "        y.append(label)\n",
        "\n",
        "        features = featurizer(url, html)\n",
        "        feature_descriptions, feature_values = zip(*features.items())\n",
        "\n",
        "        X.append(feature_values)\n",
        "\n",
        "    return X, y, feature_descriptions\n",
        "\n",
        "# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n",
        "# to lowercase).\n",
        "def get_normalized_count(html, phrase):\n",
        "    return math.log(1 + html.count(phrase.lower()))\n",
        "\n",
        "# Returns a dictionary mapping from plaintext feature descriptions to numerical\n",
        "# features for a (url, html) pair.\n",
        "def keyword_featurizer(url, html):\n",
        "    features = {}\n",
        "\n",
        "    # Same as before.\n",
        "    features['.com domain'] = url.endswith('.com')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.net domain'] = url.endswith('.net')\n",
        "    features['.info domain'] = url.endswith('.info')\n",
        "    features['.org domain'] = url.endswith('.org')\n",
        "    features['.biz domain'] = url.endswith('.biz')\n",
        "    features['.ru domain'] = url.endswith('.ru')\n",
        "    features['.co.uk domain'] = url.endswith('.co.uk')\n",
        "    features['.co domain'] = url.endswith('.co')\n",
        "    features['.tv domain'] = url.endswith('.tv')\n",
        "    features['.news domain'] = url.endswith('.news')\n",
        "\n",
        "    keywords = ['trump', 'biden', 'clinton', 'sports', 'finance']\n",
        "\n",
        "    for keyword in keywords:\n",
        "      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n",
        "\n",
        "    return features\n",
        "\n",
        "keyword_train_X, train_y, _ = prepare_data(train_data, keyword_featurizer)\n",
        "keyword_val_X, val_y, _ = prepare_data(val_data, keyword_featurizer)\n",
        "\n",
        "train_and_evaluate_model(keyword_train_X, train_y, keyword_val_X, val_y)\n"
      ],
      "metadata": {
        "id": "atxdALOYFVTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=300)\n",
        "\n",
        "vectorizer.fit(train_descriptions)\n",
        "\n",
        "def vectorize_data_descriptions(data_descriptions, vectorizer):\n",
        "  X = vectorizer.transform(data_descriptions).todense()\n",
        "  return X\n",
        "\n",
        "bow_train_X = vectorize_data_descriptions(train_descriptions, vectorizer)\n",
        "bow_val_X = vectorize_data_descriptions(val_descriptions, vectorizer)\n",
        "\n",
        "train_and_evaluate_model(bow_train_X, train_y, bow_val_X, val_y)"
      ],
      "metadata": {
        "id": "AEftqEU7FdQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VEC_SIZE = 300\n",
        "glove = GloVe(name='6B', dim=VEC_SIZE)\n",
        "\n",
        "# Returns word vector for word if it exists, else return None.\n",
        "def get_word_vector(word):\n",
        "    try:\n",
        "      return glove.vectors[glove.stoi[word.lower()]].numpy()\n",
        "    except KeyError:\n",
        "      return None\n",
        "\n",
        "def glove_transform_data_descriptions(descriptions):\n",
        "    X = np.zeros((len(descriptions), VEC_SIZE))\n",
        "    for i, description in enumerate(descriptions):\n",
        "        found_words = 0.0\n",
        "        description = description.strip()\n",
        "        for word in description.split():\n",
        "            vec = get_word_vector(word)\n",
        "            if vec is not None:\n",
        "                # Increment found_words and add vec to X[i].\n",
        "                found_words += 1\n",
        "                X[i] += vec\n",
        "        if found_words > 0:\n",
        "            X[i] /= found_words\n",
        "\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "oF1G-AjRFgZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_train_X = glove_transform_data_descriptions(train_descriptions)\n",
        "glove_val_X = glove_transform_data_descriptions(val_descriptions)\n",
        "\n",
        "train_and_evaluate_model(glove_train_X, train_y, glove_val_X, val_y)"
      ],
      "metadata": {
        "id": "DIoGp3oAFlau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_features(X_list):\n",
        "  return np.concatenate(X_list, axis=1)\n",
        "\n",
        "combined_train_X = combine_features([keyword_train_X, bow_train_X, glove_train_X])\n",
        "combined_val_X = combine_features([keyword_val_X, bow_val_X, glove_val_X])\n",
        "\n",
        "model = train_and_evaluate_model(combined_train_X, train_y, combined_val_X, val_y)"
      ],
      "metadata": {
        "id": "rm8l5znwFnKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_pair(url):\n",
        "  if not url.startswith('http'):\n",
        "      url = 'http://' + url\n",
        "  url_pretty = url\n",
        "  if url_pretty.startswith('http://'):\n",
        "      url_pretty = url_pretty[7:]\n",
        "  if url_pretty.startswith('https://'):\n",
        "      url_pretty = url_pretty[8:]\n",
        "\n",
        "  # Scrape website for HTML\n",
        "  response = requests.get(url, timeout=10)\n",
        "  htmltext = response.text\n",
        "\n",
        "  return url_pretty, htmltext\n",
        "\n",
        "curr_url = \"www.yahoo.com\"\n",
        "\n",
        "url, html = get_data_pair(curr_url)\n",
        "\n",
        "def dict_to_features(features_dict):\n",
        "  X = np.array(list(features_dict.values())).astype('float')\n",
        "  X = X[np.newaxis, :]\n",
        "  return X\n",
        "def featurize_data_pair(url, html):\n",
        "  # Approach 1.\n",
        "  keyword_X = dict_to_features(keyword_featurizer(url, html))\n",
        "  # Approach 2.\n",
        "  description = get_description_from_html(html)\n",
        "\n",
        "  bow_X = vectorize_data_descriptions([description], vectorizer)\n",
        "\n",
        "  # Approach 3.\n",
        "  glove_X = glove_transform_data_descriptions([description])\n",
        "\n",
        "  X = combine_features([keyword_X, bow_X, glove_X])\n",
        "\n",
        "  return X\n",
        "\n",
        "curr_X = featurize_data_pair(url, html)\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "curr_y = model.predict(curr_X)[0]\n",
        "\n",
        "\n",
        "if curr_y < .5:\n",
        "  print(curr_url, 'appears to be real.')\n",
        "else:\n",
        "  print(curr_url, 'appears to be fake.')"
      ],
      "metadata": {
        "id": "-lRpNosmFtDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(os.path.join(basepath, 'test_data.pkl'), 'rb') as f:\n",
        "  test_data = pickle.load(f)\n",
        "print('Number of test examples:', len(test_data))\n",
        "\n",
        "\n",
        "\n",
        "model = train_model(combined_train_X, train_y, combined_val_X, val_y)\n",
        "\n",
        "print('Loading test data...')\n",
        "test_X = []\n",
        "for url, html, label in test_data:\n",
        "  curr_X = np.array(featurize_data_pair(url, html))\n",
        "  test_X.append(curr_X[0])\n",
        "\n",
        "test_X = np.array(test_X)\n",
        "\n",
        "test_y = [label for url, html, label in test_data]\n",
        "\n",
        "print('Done loading test data...')\n",
        "\n",
        "test_y_pred = model.predict(test_X)\n",
        "\n",
        "print('Test accuracy', accuracy_score(test_y, test_y_pred))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(test_y, test_y_pred))\n",
        "\n",
        "prf = precision_recall_fscore_support(test_y, test_y_pred)\n",
        "\n",
        "print('Precision:', prf[0][1])\n",
        "print('Recall:', prf[1][1])\n",
        "print('F-Score:', prf[2][1])"
      ],
      "metadata": {
        "id": "Ib6EQs6gFxm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}